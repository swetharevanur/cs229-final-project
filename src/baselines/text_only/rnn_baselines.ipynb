{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rnn_classifier.train import train_handler\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torchtext.data as data\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Field\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediates_dir = '../../../intermediates'\n",
    "\n",
    "TRAIN_PATH = 'cleaned_text_train.csv'\n",
    "VAL_PATH = 'cleaned_text_val.csv'\n",
    "TEST_PATH = 'cleaned_text_test.csv'\n",
    "\n",
    "VOCABULARY_PATH = os.path.join(intermediates_dir, 'text_vocabulary.pkl')\n",
    "vocabulary = pickle.load(open(VOCABULARY_PATH, 'rb'))\n",
    "num_tokens = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential = True, tokenize = tokenize, lower = True)\n",
    "LABEL = Field(sequential = False, use_vocab = False)\n",
    "\n",
    "train_datafields = [('file_name', None), # not needed\n",
    "                    ('text', TEXT), \n",
    "                    ('image_loc', None),\n",
    "                    ('damaged_infrastructure', LABEL), \n",
    "                    ('damaged_nature', LABEL),\n",
    "                    ('fires', LABEL), \n",
    "                    ('flood', LABEL),\n",
    "                    ('human_damage', LABEL),\n",
    "                    ('non_damage', LABEL)]\n",
    "\n",
    "train, val = TabularDataset.splits(path = intermediates_dir, # the root directory where the data lies\n",
    "                                   train = TRAIN_PATH, \n",
    "                                   validation = VAL_PATH,\n",
    "                                   format = 'csv',\n",
    "                                   skip_header = True, \n",
    "                                   fields = train_datafields)\n",
    "\n",
    "test_datafields = [('file_name', None), # not needed\n",
    "                   ('text', TEXT), \n",
    "                   ('image_loc', None),\n",
    "                   ('damaged_infrastructure', LABEL), \n",
    "                   ('damaged_nature', LABEL),\n",
    "                   ('fires', LABEL), \n",
    "                   ('flood', LABEL),\n",
    "                   ('human_damage', LABEL),\n",
    "                   ('non_damage', LABEL)]\n",
    "\n",
    "test = TabularDataset(path = os.path.join(intermediates_dir, TEST_PATH),\n",
    "                      format = 'csv',\n",
    "                      skip_header = True,\n",
    "                      fields = test_datafields)\n",
    "\n",
    "datasets = {\n",
    "    'train': train,\n",
    "    'val': val,\n",
    "    'test': test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'rnn_type': 'LSTM', # 'LSTM', 'GRU'\n",
    "    'embedding_size': 100,\n",
    "    'num_hidden_units': 500,\n",
    "    'num_layers': 2,\n",
    "    'init_lr': 1e-3,\n",
    "    'grad_clipping': 5,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'dropout_rate': 0,\n",
    "    'is_bidirectional': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model, best_acc, train_loss_history, train_acc_history, val_acc_history = train_handler(hyperparams, datasets, TEXT, LABEL, len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join('output', 'trained_models', 'lstm_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
