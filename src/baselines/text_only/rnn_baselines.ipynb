{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rnn_classifier import model\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = \"../../cleaned_data/cleaned_text.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    rnn_type: 'LSTM', # 'LSTM', 'GRU'\n",
    "    embedding_size: 300,\n",
    "    num_hidden_units: 500,\n",
    "    num_layers: 2,\n",
    "    init_lr: 1e-3,\n",
    "    grad_clipping: 5,\n",
    "    num_epochs: 10,\n",
    "    batch_size: 32,\n",
    "    dropout_rate: 0,\n",
    "    is_bidirectional: True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset into np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "train_path = os.path.join(TRAIN_SET_PATH)\n",
    "df = pd.read_pickle(train_path)\n",
    "\n",
    "all_x = df['text']\n",
    "for x in all_x:\n",
    "    X.append(x.split())\n",
    "    \n",
    "y = df['class'].values\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "print (\"total examples %s\" % len(y))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(GLOVE_6B_50D_PATH, \"rb\") as lines:\n",
    "    wvec = {line.split()[0].decode(encoding): np.array(line.split()[1:],dtype=np.float32)\n",
    "               for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words)\n",
    "with open(GLOVE_6B_50D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums\n",
    "\n",
    "            \n",
    "glove_big = {}\n",
    "with open(GLOVE_840B_300D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if word in all_words:\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_big[word] = nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with the classics - naive bayes of the multinomial and bernoulli varieties\n",
    "# with either pure counts or tfidf features\n",
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"svm\", svc),\n",
    "    (\"svm_tfidf\", svc_tfidf),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "    (\"glove_small\", etree_glove_small),\n",
    "    (\"glove_small_tfidf\", etree_glove_small_tfidf),\n",
    "    (\"glove_big\", etree_glove_big),\n",
    "    (\"glove_big_tfidf\", etree_glove_big_tfidf),\n",
    "\n",
    "]\n",
    " \n",
    "scoring = ['precision_macro', 'recall_macro', 'accuracy']\n",
    "unsorted_scores = [(name, cross_validate(model, X, y, cv=5, scoring=scoring, return_train_score=True, return_estimator=True)) \\\n",
    "                   for name, model in all_models]\n",
    "# scores = sorted(unsorted_scores, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort table into various predictors\n",
    "table = []\n",
    "for classifier in unsorted_scores:\n",
    "    name = classifier[0]\n",
    "    classi = [name]\n",
    "    scores = classifier[1]\n",
    "    for score in scores:\n",
    "        if score != 'estimator': \n",
    "            classi.append(scores[score].mean())\n",
    "    table.append(classi)\n",
    "\n",
    "# add f1 score using the formula 2*((test_prec*test_rec)/(test_prec+test_rec))\n",
    "for classifier in table:\n",
    "    name = classifier[0]\n",
    "    test_prec = classifier[3]\n",
    "    test_rec = classifier[5]\n",
    "    test_f1 = 2*((test_prec*test_rec)/(test_prec+test_rec))\n",
    "    \n",
    "    classifier.append(test_f1)\n",
    "    \n",
    "    train_prec = classifier[4]\n",
    "    train_rec = classifier[6]\n",
    "    test_f1 = 2*((train_prec*train_rec)/(train_prec+train_rec))\n",
    "    classifier.append(test_f1)\n",
    "    \n",
    "table = sorted(table, key=lambda x: -x[7])\n",
    "print (tabulate(table, floatfmt=\".4f\", headers=(\"model\",'fit_time', 'score_time', 'test_prec', \n",
    "                                                 'train_prec', 'test_rec', 'train_rec',\n",
    "                                                 'test_acc', 'train_acc', 'test_f1', 'train_f1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newfile = '/Users/swetharevanur/Documents/3_Junior/1_Fall/cs229/cs229-final-project/intermediates/cleaned_text_total.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(newfile)\n",
    "\n",
    "# import _pickle as cPickle\n",
    "# with open(newfile, 'rb') as fo:\n",
    "#     dict_test = cPickle.load(fo, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      file_name  \\\n",
      "0            isiscrimes_2015-08-23_21-07-05.txt   \n",
      "1        syriawarcrimes_2017-06-19_06-24-25.txt   \n",
      "2            isiscrimes_2015-08-27_00-07-30.txt   \n",
      "3            isiscrimes_2015-08-11_01-05-40.txt   \n",
      "4           yemencrisis_2015-11-27_01-40-28.txt   \n",
      "5            isiscrimes_2015-08-03_11-18-40.txt   \n",
      "6            isiscrimes_2015-04-26_00-37-04.txt   \n",
      "7          terrorattack_2017-10-02_13-13-23.txt   \n",
      "8            isiscrimes_2015-08-01_10-54-18.txt   \n",
      "9          victimsofwar_2016-12-17_02-12-27.txt   \n",
      "10           isiscrimes_2015-08-01_10-57-48.txt   \n",
      "11           isiscrimes_2015-08-14_23-22-03.txt   \n",
      "12           isiscrimes_2015-08-16_22-00-33.txt   \n",
      "13           isiscrimes_2015-08-13_05-42-23.txt   \n",
      "14         victimsofwar_2017-03-07_19-29-35.txt   \n",
      "15          yemencrisis_2015-07-20_17-18-00.txt   \n",
      "16           isiscrimes_2015-12-09_21-02-34.txt   \n",
      "17           isiscrimes_2015-08-05_19-58-42.txt   \n",
      "18           isiscrimes_2015-12-10_17-27-35.txt   \n",
      "19         victimsofwar_2016-12-01_20-20-54.txt   \n",
      "20           isiscrimes_2015-08-02_17-07-03.txt   \n",
      "21           isiscrimes_2015-08-23_06-53-41.txt   \n",
      "22           isiscrimes_2015-08-27_00-11-43.txt   \n",
      "23          yemencrisis_2017-07-31_06-24-52.txt   \n",
      "24          yemencrisis_2017-01-14_13-54-14.txt   \n",
      "25          yemencrisis_2017-02-01_21-18-28.txt   \n",
      "26         terrorattack_2017-10-02_18-29-18.txt   \n",
      "27           isiscrimes_2015-07-26_00-50-27.txt   \n",
      "28           isiscrimes_2015-11-20_14-34-12.txt   \n",
      "29           isiscrimes_2015-08-22_02-31-53.txt   \n",
      "...                                         ...   \n",
      "5801      hurricaneirma_2017-11-17_00-49-44.txt   \n",
      "5802         wreckedcar_2017-07-06_19-11-52.txt   \n",
      "5803         floodwater_2017-09-13_02-50-47.txt   \n",
      "5804         wreckedcar_2017-06-15_11-32-35.txt   \n",
      "5805          disasters_2017-11-23_04-08-41.txt   \n",
      "5806         wreckedcar_2017-10-23_22-00-43.txt   \n",
      "5807        sandydamage_2012-10-31_06-28-06.txt   \n",
      "5808         wreckedcar_2017-05-27_21-12-05.txt   \n",
      "5809     hurricanesandy_2017-10-29_14-17-28.txt   \n",
      "5810        destruction_2017-10-30_10-18-23.txt   \n",
      "5811         earthquake_2017-11-13_00-18-43.txt   \n",
      "5812         wreckedcar_2017-10-24_09-52-11.txt   \n",
      "5813   buildingcollapse_2014-03-12_16-41-42.txt   \n",
      "5814         earthquake_2017-11-13_19-14-40.txt   \n",
      "5815         wreckedcar_2017-10-01_00-20-17.txt   \n",
      "5816    naturaldisaster_2017-10-17_22-51-56.txt   \n",
      "5817          disasters_2017-11-08_16-20-04.txt   \n",
      "5818   buildingcollapse_2017-07-25_14-52-15.txt   \n",
      "5819     hurricanesandy_2017-10-29_20-14-27.txt   \n",
      "5820      hurricaneirma_2017-11-16_21-44-02.txt   \n",
      "5821    naturaldisaster_2017-10-18_12-22-09.txt   \n",
      "5822        destruction_2017-10-28_00-03-31.txt   \n",
      "5823         wreckedcar_2017-10-25_17-48-15.txt   \n",
      "5824        sandydamage_2012-11-18_12-33-37.txt   \n",
      "5825          disasters_2017-11-21_06-23-49.txt   \n",
      "5826         wreckedcar_2017-08-05_10-23-52.txt   \n",
      "5827  destroyedbuilding_2012-08-08_15-03-14.txt   \n",
      "5828         wreckedcar_2017-09-14_01-50-20.txt   \n",
      "5829        destruction_2017-10-27_22-31-35.txt   \n",
      "5830         wreckedcar_2017-08-03_17-09-35.txt   \n",
      "\n",
      "                                                   text  \\\n",
      "0     how they welcome syrian refugees in macedonia ...   \n",
      "1     when the coalition is no different than assad ...   \n",
      "2     syrian children are the primary target of syri...   \n",
      "3     syrian genocide continuesassadcrimes isiscrime...   \n",
      "4     this is yemen child buried under rubble as 90 ...   \n",
      "5     17 children were killed this month alone by as...   \n",
      "6     yemen girls that look like this are being stol...   \n",
      "7     lasvegas shooting two dead 24 wounded by gunfi...   \n",
      "8     every day syrian children are being injured or...   \n",
      "9         victimsofwar reflection emotionalintelligence   \n",
      "10    every day syrian children are being injured or...   \n",
      "11    syrians are bleedingsyrians syria syrie syrian...   \n",
      "12    syrian regime committed a new massacre today i...   \n",
      "13    syria today the red color is the dominant colo...   \n",
      "14    i didnt rate this picture at the time i took i...   \n",
      "15    Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„ÙŠÙ…Ù†ÙŠ Ø§Ù„Ø­Ø±â€¦ Ø§Ù„Ø±Ø¬ÙˆÙ„Ù‡ Ø§Ù„Ù†Ø®ÙˆØ©â€¦ ÙˆÙ‡Ø°Ø§ Ù‡Ùˆ Ø¨ÙŠ...   \n",
      "16    russian regime targeting syrian childrenshameo...   \n",
      "17    red is the true friend of syrian red the color...   \n",
      "18    russian regime headed by vladimir putin are ta...   \n",
      "19    tune in to newshour tonight to see two young a...   \n",
      "20    dont take my pictureim living a miserable life...   \n",
      "21    this is a real picture not a photoshopthe syri...   \n",
      "22    syrian children are the primary target of syri...   \n",
      "23    regrann from hanifahandini   saudi war crimes ...   \n",
      "24    Ø§Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ ØªØ¹Ø² Ùˆ Ø¹Ù„ÙŠ Ù…Ù† Ø¨Ø§Ø¹Ùƒ Ø¨Ø§ Ø§Ø±Ø®Øµ Ø«Ù…Ù† Ùˆ Ù†Ø´Ø±...   \n",
      "25    ØµÙ†Ø¹Ø§Ø¡Ø§Ø±ØªÙØ§Ø¹ Ø­ØµÙŠÙ„Ø© Ø¶Ø­Ø§ÙŠØ§ ØºØ§Ø±Ø§Øª Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„ØªØ­Ø§Ù„ÙØ§Ù„Ø³...   \n",
      "26    reposting infiafact with instarepostapp  sejau...   \n",
      "27    syrian regime and isis are destroying syria bo...   \n",
      "28    just imagine please just imagine that you are ...   \n",
      "29    the syrian regime bombardment does not stop ev...   \n",
      "...                                                 ...   \n",
      "5801  lifeasatraveller miami bienvenidoamiami coconu...   \n",
      "5802  riiigghtt\\n\\n\\n\\n\\n\\n\\nnissan meme memes wreck...   \n",
      "5803  what a wise man and a wonderful life lesson al...   \n",
      "5804  bodie bodiecalifornia bodieghosttown californi...   \n",
      "5805  photos from montego bay flooding ğŸ‡¯ğŸ‡² prayformob...   \n",
      "5806  ek holden ute 196162 holdenute holden cooberpe...   \n",
      "5807  manhattan nyc hurricane sandy steals facade of...   \n",
      "5808  good times with my brothers goldshift wreckedc...   \n",
      "5809  5 years ago this was my street where i lived h...   \n",
      "5810  fallen tree and destroyed traffic lights after...   \n",
      "5811  archive 2 2001 at 0846am on the morning of 26t...   \n",
      "5812  wreckedcar car carwreck truck tir road travel ...   \n",
      "5813          before and after newyork buildingcollapse   \n",
      "5814  ØªØ³Ù„ÛŒØª Ø®ÛŒÙ„ÛŒ Ú©Ù„Ù…Ù‡ ÛŒ Ø¨ÛŒ Ø§Ø±Ø²Ø´ÛŒÙ‡ ÙˆÙ‚ØªÛŒ ÛŒÚ©ÛŒ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ùˆ...   \n",
      "5815  abandoned but not forgotten from nature plant ...   \n",
      "5816  silver skies ğŸŒ\\n\\n\\n\\nstormophelia naturaldisa...   \n",
      "5817  new episode it was just supposed to be a fun n...   \n",
      "5818  building collapse in ghatkoparğŸ˜“\\n\\nat least fi...   \n",
      "5819                    destiny of sandy hurricanesandy   \n",
      "5820  cuba habana day after hurricane irma irma hura...   \n",
      "5821  this is what the costuless grocery store in st...   \n",
      "5822  destruction construction underconstruction tel...   \n",
      "5823  ÏŒÏ„Î±Î½ Î¼ÎµÏ„Î¬ Ï„Î· Î²ÏŒÎ»Ï„Î± Î»ÎµÏ‚ ÏƒÏ„Î¿Î½ Ï€Î±Ï„Î­ÏÎ± ÏƒÎ¿Ï… ÏŒÏ„Î¹ Ï„Î¿ ...   \n",
      "5824             sandyaftermathsandydamage statenisland   \n",
      "5825  disaster of november 2017\\n carbon fiber hood ...   \n",
      "5826  is it the apocalypse or a burnt out stolen car...   \n",
      "5827                                  destroyedbuilding   \n",
      "5828  nissan nissanaltima  wrecked wreckedit wrecked...   \n",
      "5829  restos de una epoca art photography picture bl...   \n",
      "5830  oldhouse yellow wreckedcar  abandoned stmarys ...   \n",
      "\n",
      "                                              image_loc  \\\n",
      "0     ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "1     ../data/human_damage/images/syriawarcrimes_201...   \n",
      "2     ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "3     ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "4     ../data/human_damage/images/yemencrisis_2015-1...   \n",
      "5     ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "6     ../data/human_damage/images/isiscrimes_2015-04...   \n",
      "7     ../data/human_damage/images/terrorattack_2017-...   \n",
      "8     ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "9     ../data/human_damage/images/victimsofwar_2016-...   \n",
      "10    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "11    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "12    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "13    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "14    ../data/human_damage/images/victimsofwar_2017-...   \n",
      "15    ../data/human_damage/images/yemencrisis_2015-0...   \n",
      "16    ../data/human_damage/images/isiscrimes_2015-12...   \n",
      "17    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "18    ../data/human_damage/images/isiscrimes_2015-12...   \n",
      "19    ../data/human_damage/images/victimsofwar_2016-...   \n",
      "20    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "21    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "22    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "23    ../data/human_damage/images/yemencrisis_2017-0...   \n",
      "24    ../data/human_damage/images/yemencrisis_2017-0...   \n",
      "25    ../data/human_damage/images/yemencrisis_2017-0...   \n",
      "26    ../data/human_damage/images/terrorattack_2017-...   \n",
      "27    ../data/human_damage/images/isiscrimes_2015-07...   \n",
      "28    ../data/human_damage/images/isiscrimes_2015-11...   \n",
      "29    ../data/human_damage/images/isiscrimes_2015-08...   \n",
      "...                                                 ...   \n",
      "5801  ../data/damaged_infrastructure/images/hurrican...   \n",
      "5802  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5803  ../data/damaged_infrastructure/images/floodwat...   \n",
      "5804  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5805  ../data/damaged_infrastructure/images/disaster...   \n",
      "5806  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5807  ../data/damaged_infrastructure/images/sandydam...   \n",
      "5808  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5809  ../data/damaged_infrastructure/images/hurrican...   \n",
      "5810  ../data/damaged_infrastructure/images/destruct...   \n",
      "5811  ../data/damaged_infrastructure/images/earthqua...   \n",
      "5812  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5813  ../data/damaged_infrastructure/images/building...   \n",
      "5814  ../data/damaged_infrastructure/images/earthqua...   \n",
      "5815  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5816  ../data/damaged_infrastructure/images/naturald...   \n",
      "5817  ../data/damaged_infrastructure/images/disaster...   \n",
      "5818  ../data/damaged_infrastructure/images/building...   \n",
      "5819  ../data/damaged_infrastructure/images/hurrican...   \n",
      "5820  ../data/damaged_infrastructure/images/hurrican...   \n",
      "5821  ../data/damaged_infrastructure/images/naturald...   \n",
      "5822  ../data/damaged_infrastructure/images/destruct...   \n",
      "5823  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5824  ../data/damaged_infrastructure/images/sandydam...   \n",
      "5825  ../data/damaged_infrastructure/images/disaster...   \n",
      "5826  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5827  ../data/damaged_infrastructure/images/destroye...   \n",
      "5828  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "5829  ../data/damaged_infrastructure/images/destruct...   \n",
      "5830  ../data/damaged_infrastructure/images/wreckedc...   \n",
      "\n",
      "                       class  \n",
      "0               human_damage  \n",
      "1               human_damage  \n",
      "2               human_damage  \n",
      "3               human_damage  \n",
      "4               human_damage  \n",
      "5               human_damage  \n",
      "6               human_damage  \n",
      "7               human_damage  \n",
      "8               human_damage  \n",
      "9               human_damage  \n",
      "10              human_damage  \n",
      "11              human_damage  \n",
      "12              human_damage  \n",
      "13              human_damage  \n",
      "14              human_damage  \n",
      "15              human_damage  \n",
      "16              human_damage  \n",
      "17              human_damage  \n",
      "18              human_damage  \n",
      "19              human_damage  \n",
      "20              human_damage  \n",
      "21              human_damage  \n",
      "22              human_damage  \n",
      "23              human_damage  \n",
      "24              human_damage  \n",
      "25              human_damage  \n",
      "26              human_damage  \n",
      "27              human_damage  \n",
      "28              human_damage  \n",
      "29              human_damage  \n",
      "...                      ...  \n",
      "5801  damaged_infrastructure  \n",
      "5802  damaged_infrastructure  \n",
      "5803  damaged_infrastructure  \n",
      "5804  damaged_infrastructure  \n",
      "5805  damaged_infrastructure  \n",
      "5806  damaged_infrastructure  \n",
      "5807  damaged_infrastructure  \n",
      "5808  damaged_infrastructure  \n",
      "5809  damaged_infrastructure  \n",
      "5810  damaged_infrastructure  \n",
      "5811  damaged_infrastructure  \n",
      "5812  damaged_infrastructure  \n",
      "5813  damaged_infrastructure  \n",
      "5814  damaged_infrastructure  \n",
      "5815  damaged_infrastructure  \n",
      "5816  damaged_infrastructure  \n",
      "5817  damaged_infrastructure  \n",
      "5818  damaged_infrastructure  \n",
      "5819  damaged_infrastructure  \n",
      "5820  damaged_infrastructure  \n",
      "5821  damaged_infrastructure  \n",
      "5822  damaged_infrastructure  \n",
      "5823  damaged_infrastructure  \n",
      "5824  damaged_infrastructure  \n",
      "5825  damaged_infrastructure  \n",
      "5826  damaged_infrastructure  \n",
      "5827  damaged_infrastructure  \n",
      "5828  damaged_infrastructure  \n",
      "5829  damaged_infrastructure  \n",
      "5830  damaged_infrastructure  \n",
      "\n",
      "[5831 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
