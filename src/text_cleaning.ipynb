{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "BASE_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text Files into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals to store data\n",
    "df = pd.DataFrame(columns=['file_name', 'text', 'image_loc', 'class'])\n",
    "vocabulary = set()\n",
    "\n",
    "# translate text from post \n",
    "def translate(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # remove lower case\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we iterate through all of the class files and extract the text data and store the posts in a data frame with their corresponding labels and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate through file dataset and extract text data\n",
    "for class_name in os.listdir(BASE_DIR):\n",
    "    text_dir = os.path.join(BASE_DIR, class_name, 'text/')\n",
    "    img_dir = os.path.join(BASE_DIR, class_name, 'images/')\n",
    "    if os.path.isdir(text_dir):\n",
    "        for text_name in os.listdir(text_dir):\n",
    "            # construct image file name\n",
    "            image_file = os.path.join(BASE_DIR, class_name, 'images/', text_name[:-4] + '.jpg')\n",
    "\n",
    "            # get text data\n",
    "            file = os.path.join(BASE_DIR, class_name, 'text', text_name)\n",
    "            f = open(file, 'r') \n",
    "            text = translate(f.read())\n",
    "            vocabulary.update(set(text.split()))\n",
    "\n",
    "            # append to dataset\n",
    "            df = df.append({'file_name': text_name, \n",
    "                       'text': text, \n",
    "                       'image_loc': image_file, \n",
    "                       'class': class_name}, \n",
    "                      ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all of the text data and vocabulary list in pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_textf = os.path.join('../cleaned_data', 'cleaned_text.pkl')\n",
    "path_to_vocabf = os.path.join('../cleaned_data', 'text_vocabulary.pkl')\n",
    "df.to_pickle(path_to_textf)\n",
    "with open(path_to_vocabf, 'wb') as pickle_file:\n",
    "    pickle.dump(sorted(vocabulary), pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
