{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "BASE_DIR = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text Files into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals to store data\n",
    "train_df = pd.DataFrame(columns=['file_name', 'text', 'image_loc', 'class'])\n",
    "test_df = pd.DataFrame(columns=['file_name', 'text', 'image_loc', 'class'])\n",
    "val_df = pd.DataFrame(columns=['file_name', 'text', 'image_loc', 'class'])\n",
    "vocabulary = set()\n",
    "\n",
    "\n",
    "# translate text from post \n",
    "def translate(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # remove lower case\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we iterate through all of the class files and extract the text data and store the posts in a data frame with their corresponding labels and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate through file dataset and extract text data\n",
    "text_data_fn = os.path.join(BASE_DIR, 'text_data')\n",
    "img_data_fn = os.path.join(BASE_DIR, 'img_data')\n",
    "for split in os.listdir(text_data_fn):\n",
    "    split_dir = os.path.join(text_data_fn, split)\n",
    "    if os.path.isdir(split_dir):\n",
    "        for class_name in os.listdir(split_dir):\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for text_name in os.listdir(class_dir):\n",
    "                    # construct image file name\n",
    "                    image_file = os.path.join(img_data_fn, split, class_name, text_name[:-4] + '.jpg')\n",
    "\n",
    "                    # get text data\n",
    "                    text_file = os.path.join(class_dir, text_name)\n",
    "                    f = open(text_file, 'r') \n",
    "                    text = translate(f.read())\n",
    "                    vocabulary.update(set(text.split()))\n",
    "\n",
    "                    # append to appropriate dataset\n",
    "                    if split == 'train': #df = train_df\n",
    "                        train_df = train_df.append({'file_name': text_name, \n",
    "                               'text': text, \n",
    "                               'image_loc': image_file, \n",
    "                               'class': class_name}, \n",
    "                              ignore_index=True)\n",
    "                    if split == 'train': #df = train_df\n",
    "                        val_df = val_df.append({'file_name': text_name, \n",
    "                               'text': text, \n",
    "                               'image_loc': image_file, \n",
    "                               'class': class_name}, \n",
    "                              ignore_index=True)\n",
    "                    else:\n",
    "                        test_df = test_df.append({'file_name': text_name, \n",
    "                               'text': text, \n",
    "                               'image_loc': image_file, \n",
    "                               'class': class_name}, \n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all of the text data and vocabulary list in pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_textf_train = os.path.join('../intermediates', 'cleaned_text_train.pkl')\n",
    "path_to_textf_test = os.path.join('../intermediates', 'cleaned_text_test.pkl')\n",
    "path_to_textf_val = os.path.join('../intermediates', 'cleaned_text_val.pkl')\n",
    "train_df.to_pickle(path_to_textf_train)\n",
    "test_df.to_pickle(path_to_textf_test)\n",
    "val_df.to_pickle(path_to_textf_val)\n",
    "\n",
    "path_to_vocabf = os.path.join('../intermediates', 'text_vocabulary.pkl')\n",
    "with open(path_to_vocabf, 'wb') as pickle_file:\n",
    "    pickle.dump(sorted(vocabulary), pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      file_name  \\\n",
      "0           yemencrisis_2015-11-27_01-40-28.txt   \n",
      "1            isiscrimes_2015-08-03_11-18-40.txt   \n",
      "2          terrorattack_2017-10-02_13-13-23.txt   \n",
      "3          victimsofwar_2016-12-17_02-12-27.txt   \n",
      "4            isiscrimes_2015-08-01_10-57-48.txt   \n",
      "5            isiscrimes_2015-08-13_05-42-23.txt   \n",
      "6            isiscrimes_2015-08-05_19-58-42.txt   \n",
      "7            isiscrimes_2015-12-10_17-27-35.txt   \n",
      "8            isiscrimes_2015-08-02_17-07-03.txt   \n",
      "9            isiscrimes_2015-08-23_06-53-41.txt   \n",
      "10           isiscrimes_2015-08-27_00-11-43.txt   \n",
      "11          yemencrisis_2017-07-31_06-24-52.txt   \n",
      "12         terrorattack_2017-10-02_18-29-18.txt   \n",
      "13           isiscrimes_2015-11-20_14-34-12.txt   \n",
      "14           isiscrimes_2015-08-22_02-31-53.txt   \n",
      "15           isiscrimes_2015-12-12_22-49-40.txt   \n",
      "16           isiscrimes_2015-07-25_13-04-32.txt   \n",
      "17           isiscrimes_2015-12-12_18-52-25.txt   \n",
      "18           earthquake_2017-11-13_19-05-32.txt   \n",
      "19         victimsofwar_2017-01-05_09-26-08.txt   \n",
      "20           isiscrimes_2015-08-09_15-35-54.txt   \n",
      "21           isiscrimes_2015-08-29_18-51-40.txt   \n",
      "22         terrorattack_2017-11-01_01-10-23.txt   \n",
      "23           isiscrimes_2015-08-04_00-17-25.txt   \n",
      "24           isiscrimes_2015-08-13_17-26-29.txt   \n",
      "25           isiscrimes_2015-08-10_06-10-13.txt   \n",
      "26           isiscrimes_2015-08-13_15-53-32.txt   \n",
      "27         victimsofwar_2017-07-07_16-56-09.txt   \n",
      "28           isiscrimes_2015-08-31_19-49-33.txt   \n",
      "29           isiscrimes_2015-08-13_19-32-42.txt   \n",
      "...                                         ...   \n",
      "6968         wreckedcar_2017-10-26_00-28-50.txt   \n",
      "6969  destroyedbuilding_2017-03-18_18-35-11.txt   \n",
      "6970         wreckedcar_2016-12-04_18-33-02.txt   \n",
      "6971         earthquake_2017-11-09_18-57-13.txt   \n",
      "6972         wreckedcar_2017-08-13_06-49-28.txt   \n",
      "6973         wreckedcar_2016-12-15_10-05-06.txt   \n",
      "6974    naturaldisaster_2017-10-26_14-34-14.txt   \n",
      "6975         wreckedcar_2017-10-22_13-15-08.txt   \n",
      "6976         earthquake_2017-11-13_19-05-11.txt   \n",
      "6977         wreckedcar_2017-07-10_22-44-50.txt   \n",
      "6978        accrafloods_2015-06-15_23-01-39.txt   \n",
      "6979         wreckedcar_2016-12-11_15-21-00.txt   \n",
      "6980         wreckedcar_2017-08-06_19-42-24.txt   \n",
      "6981      hurricaneirma_2017-11-17_00-49-44.txt   \n",
      "6982         wreckedcar_2017-07-06_19-11-52.txt   \n",
      "6983         wreckedcar_2017-06-15_11-32-35.txt   \n",
      "6984         wreckedcar_2017-10-23_22-00-43.txt   \n",
      "6985        sandydamage_2012-10-31_06-28-06.txt   \n",
      "6986     hurricanesandy_2017-10-29_14-17-28.txt   \n",
      "6987        destruction_2017-10-30_10-18-23.txt   \n",
      "6988         wreckedcar_2017-10-24_09-52-11.txt   \n",
      "6989   buildingcollapse_2014-03-12_16-41-42.txt   \n",
      "6990         earthquake_2017-11-13_19-14-40.txt   \n",
      "6991    naturaldisaster_2017-10-17_22-51-56.txt   \n",
      "6992   buildingcollapse_2017-07-25_14-52-15.txt   \n",
      "6993      hurricaneirma_2017-11-16_21-44-02.txt   \n",
      "6994        destruction_2017-10-28_00-03-31.txt   \n",
      "6995         wreckedcar_2017-10-25_17-48-15.txt   \n",
      "6996        sandydamage_2012-11-18_12-33-37.txt   \n",
      "6997         wreckedcar_2017-08-05_10-23-52.txt   \n",
      "\n",
      "                                                   text  \\\n",
      "0     this is yemen child buried under rubble as 90 ...   \n",
      "1     17 children were killed this month alone by as...   \n",
      "2     lasvegas shooting two dead 24 wounded by gunfi...   \n",
      "3         victimsofwar reflection emotionalintelligence   \n",
      "4     every day syrian children are being injured or...   \n",
      "5     syria today the red color is the dominant colo...   \n",
      "6     red is the true friend of syrian red the color...   \n",
      "7     russian regime headed by vladimir putin are ta...   \n",
      "8     dont take my pictureim living a miserable life...   \n",
      "9     this is a real picture not a photoshopthe syri...   \n",
      "10    syrian children are the primary target of syri...   \n",
      "11    regrann from hanifahandini   saudi war crimes ...   \n",
      "12    reposting infiafact with instarepostapp  sejau...   \n",
      "13    just imagine please just imagine that you are ...   \n",
      "14    the syrian regime bombardment does not stop ev...   \n",
      "15    why mr vladimir putin the president of russia ...   \n",
      "16    every child killed by isis or assad regime is ...   \n",
      "17    why mr vladimir putin the president of russia ...   \n",
      "18     earthquake  prayforiran 🇮🇷 😢 🙇\\nزلزله هموطنتسلیت   \n",
      "19    yemens children starve as war drags on\\n📌\\n📌\\n...   \n",
      "20    tourists sitting on one of the greek island en...   \n",
      "21    syrian refugees traveling between european cou...   \n",
      "22    suspect identified in nyc terrorattack that le...   \n",
      "23    27 person were killed yesterday near idlib nor...   \n",
      "24    the syrian genocide continuesassadcrimes isisc...   \n",
      "25    good night world and humanitydont forget the s...   \n",
      "26    good morning world good morning humanitysyrian...   \n",
      "27    can life ever be the same again\\nfor someone w...   \n",
      "28    as i look at this picture my mind searches for...   \n",
      "29    syrian are suffering from the organized crime ...   \n",
      "...                                                 ...   \n",
      "6968  towtruck towlife flatbed lexus caraccident wre...   \n",
      "6969  rubble destroyedbuilding pic pictures photo ph...   \n",
      "6970           wreck wrecked wreckedcar extra streetart   \n",
      "6971  life after the 2015 nepal earthquake this is a...   \n",
      "6972  fordexpedition fordf150 suv diorama scalecar g...   \n",
      "6973                                 oldcar  wreckedcar   \n",
      "6974               may the bridges i burn light the way   \n",
      "6975  wrecked ambassador\\nproduction19582014\\ntype4 ...   \n",
      "6976            iran earthquake\\nزلزله منطقه سر پل ذهاب   \n",
      "6977  burned  rustedrustofourworldwreckedcarbeachpol...   \n",
      "6978  and this is the river thats supposed to take t...   \n",
      "6979  techniques  teamwork that counts💪\\n \\nworkhard...   \n",
      "6980                hyundai dayzautobodyshop wreckedcar   \n",
      "6981  lifeasatraveller miami bienvenidoamiami coconu...   \n",
      "6982  riiigghtt\\n\\n\\n\\n\\n\\n\\nnissan meme memes wreck...   \n",
      "6983  bodie bodiecalifornia bodieghosttown californi...   \n",
      "6984  ek holden ute 196162 holdenute holden cooberpe...   \n",
      "6985  manhattan nyc hurricane sandy steals facade of...   \n",
      "6986  5 years ago this was my street where i lived h...   \n",
      "6987  fallen tree and destroyed traffic lights after...   \n",
      "6988  wreckedcar car carwreck truck tir road travel ...   \n",
      "6989          before and after newyork buildingcollapse   \n",
      "6990  تسلیت خیلی کلمه ی بی ارزشیه وقتی یکی همه چیز و...   \n",
      "6991  silver skies 🌍\\n\\n\\n\\nstormophelia naturaldisa...   \n",
      "6992  building collapse in ghatkopar😓\\n\\nat least fi...   \n",
      "6993  cuba habana day after hurricane irma irma hura...   \n",
      "6994  destruction construction underconstruction tel...   \n",
      "6995  όταν μετά τη βόλτα λες στον πατέρα σου ότι το ...   \n",
      "6996             sandyaftermathsandydamage statenisland   \n",
      "6997  is it the apocalypse or a burnt out stolen car...   \n",
      "\n",
      "                                              image_loc  \\\n",
      "0     ../data/img_data/train/human_damage/yemencrisi...   \n",
      "1     ../data/img_data/train/human_damage/isiscrimes...   \n",
      "2     ../data/img_data/train/human_damage/terroratta...   \n",
      "3     ../data/img_data/train/human_damage/victimsofw...   \n",
      "4     ../data/img_data/train/human_damage/isiscrimes...   \n",
      "5     ../data/img_data/train/human_damage/isiscrimes...   \n",
      "6     ../data/img_data/train/human_damage/isiscrimes...   \n",
      "7     ../data/img_data/train/human_damage/isiscrimes...   \n",
      "8     ../data/img_data/train/human_damage/isiscrimes...   \n",
      "9     ../data/img_data/train/human_damage/isiscrimes...   \n",
      "10    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "11    ../data/img_data/train/human_damage/yemencrisi...   \n",
      "12    ../data/img_data/train/human_damage/terroratta...   \n",
      "13    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "14    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "15    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "16    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "17    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "18    ../data/img_data/train/human_damage/earthquake...   \n",
      "19    ../data/img_data/train/human_damage/victimsofw...   \n",
      "20    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "21    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "22    ../data/img_data/train/human_damage/terroratta...   \n",
      "23    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "24    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "25    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "26    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "27    ../data/img_data/train/human_damage/victimsofw...   \n",
      "28    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "29    ../data/img_data/train/human_damage/isiscrimes...   \n",
      "...                                                 ...   \n",
      "6968  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6969  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6970  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6971  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6972  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6973  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6974  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6975  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6976  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6977  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6978  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6979  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6980  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6981  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6982  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6983  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6984  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6985  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6986  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6987  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6988  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6989  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6990  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6991  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6992  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6993  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6994  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6995  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6996  ../data/img_data/train/damaged_infrastructure/...   \n",
      "6997  ../data/img_data/train/damaged_infrastructure/...   \n",
      "\n",
      "                       class  \n",
      "0               human_damage  \n",
      "1               human_damage  \n",
      "2               human_damage  \n",
      "3               human_damage  \n",
      "4               human_damage  \n",
      "5               human_damage  \n",
      "6               human_damage  \n",
      "7               human_damage  \n",
      "8               human_damage  \n",
      "9               human_damage  \n",
      "10              human_damage  \n",
      "11              human_damage  \n",
      "12              human_damage  \n",
      "13              human_damage  \n",
      "14              human_damage  \n",
      "15              human_damage  \n",
      "16              human_damage  \n",
      "17              human_damage  \n",
      "18              human_damage  \n",
      "19              human_damage  \n",
      "20              human_damage  \n",
      "21              human_damage  \n",
      "22              human_damage  \n",
      "23              human_damage  \n",
      "24              human_damage  \n",
      "25              human_damage  \n",
      "26              human_damage  \n",
      "27              human_damage  \n",
      "28              human_damage  \n",
      "29              human_damage  \n",
      "...                      ...  \n",
      "6968  damaged_infrastructure  \n",
      "6969  damaged_infrastructure  \n",
      "6970  damaged_infrastructure  \n",
      "6971  damaged_infrastructure  \n",
      "6972  damaged_infrastructure  \n",
      "6973  damaged_infrastructure  \n",
      "6974  damaged_infrastructure  \n",
      "6975  damaged_infrastructure  \n",
      "6976  damaged_infrastructure  \n",
      "6977  damaged_infrastructure  \n",
      "6978  damaged_infrastructure  \n",
      "6979  damaged_infrastructure  \n",
      "6980  damaged_infrastructure  \n",
      "6981  damaged_infrastructure  \n",
      "6982  damaged_infrastructure  \n",
      "6983  damaged_infrastructure  \n",
      "6984  damaged_infrastructure  \n",
      "6985  damaged_infrastructure  \n",
      "6986  damaged_infrastructure  \n",
      "6987  damaged_infrastructure  \n",
      "6988  damaged_infrastructure  \n",
      "6989  damaged_infrastructure  \n",
      "6990  damaged_infrastructure  \n",
      "6991  damaged_infrastructure  \n",
      "6992  damaged_infrastructure  \n",
      "6993  damaged_infrastructure  \n",
      "6994  damaged_infrastructure  \n",
      "6995  damaged_infrastructure  \n",
      "6996  damaged_infrastructure  \n",
      "6997  damaged_infrastructure  \n",
      "\n",
      "[6998 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
