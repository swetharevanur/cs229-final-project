{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "BASE_DIR = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text Files into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals to store data\n",
    "train_df = pd.DataFrame(columns=['file_name', 'text', 'image_loc', 'class'])\n",
    "test_df = pd.DataFrame(columns=['file_name', 'text', 'image_loc', 'class'])\n",
    "val_df = pd.DataFrame(columns=['file_name', 'text', 'image_loc', 'class'])\n",
    "vocabulary = set()\n",
    "\n",
    "# translate text from post \n",
    "def translate(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # remove lower case\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we iterate through all of the class files and extract the text data and store the posts in a data frame with their corresponding labels and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate through file dataset and extract text data\n",
    "text_data_fn = os.path.join(BASE_DIR, 'text_data')\n",
    "img_data_fn = os.path.join(BASE_DIR, 'img_data')\n",
    "for split in os.listdir(text_data_fn):\n",
    "    split_dir = os.path.join(text_data_fn, split)\n",
    "    if os.path.isdir(split_dir):\n",
    "        for class_name in os.listdir(split_dir):\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for text_name in os.listdir(class_dir):\n",
    "                    # construct image file name\n",
    "                    image_file = os.path.join(img_data_fn, split, class_name, text_name[:-4] + '.jpg')\n",
    "\n",
    "                    # get text data\n",
    "                    text_file = os.path.join(class_dir, text_name)\n",
    "                    f = open(text_file, 'r') \n",
    "                    text = translate(f.read())\n",
    "                    vocabulary.update(set(text.split()))\n",
    "\n",
    "                    # append to appropriate dataset\n",
    "                    if split == 'train': df = train_df\n",
    "                    elif split == 'test' : df = test_df\n",
    "                    else: df = val_df\n",
    "                        \n",
    "                    df = df.append({'file_name': text_name, \n",
    "                               'text': text, \n",
    "                               'image_loc': image_file, \n",
    "                               'class': class_name}, \n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all of the text data and vocabulary list in pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_textf_train = os.path.join('../cleaned_data', 'cleaned_text_train.pkl')\n",
    "path_to_textf_test = os.path.join('../cleaned_data', 'cleaned_text_test.pkl')\n",
    "path_to_textf_val = os.path.join('../cleaned_data', 'cleaned_text_val.pkl')\n",
    "train_df.to_pickle(path_to_textf_train)\n",
    "test_df.to_pickle(path_to_textf_test)\n",
    "val_df.to_pickle(path_to_textf_val)\n",
    "\n",
    "path_to_vocabf = os.path.join('../cleaned_data', 'text_vocabulary.pkl')\n",
    "with open(path_to_vocabf, 'wb') as pickle_file:\n",
    "    pickle.dump(sorted(vocabulary), pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
